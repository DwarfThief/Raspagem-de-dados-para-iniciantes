{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coletando mais detalhes\n",
    "\n",
    "Nós ja coletamos frases, o autor e suas categorias. Mas isso não é tudo que nossa Spider pode fazer, mas ainda tem coisa que nós não pegamos do site. Vamos voltar para o [site](http://quotes.toscrape.com/), note que do lado de cada autor tem o `(about)`, esse botão nos leva para uma parte do site onde fala sobre o autor. Então vamos coletar isso também.\n",
    "\n",
    "[](https://media.giphy.com/media/l1IY0geomfz09dEB2/giphy.gif)\n",
    "\n",
    "## Hora de inspecionar!\n",
    "\n",
    "Ao inspecionar o botão vamos ver que ele fica dentro de um `<a>` que fica dentro de um `<span>` que fica dentro de um `<div class=\"quote\">` e que no final de tudo, ele da um `href` pra gente... Rolou um Déjà vu? Sim, nos ja vimos isso quando aprendemos sobre [Navegação entre páginas](https://github.com/DwarfThief/Raspagem-de-dados-para-iniciantes/blob/master/Navegando%20entre%20paginas.ipynb), então vamos fazer o mesmo.\n",
    "\n",
    "**1. Vamos criar uma só para extrair esses detalhes. É só fazer `scrapy genspider autores toscrape.com/`.**\n",
    "\n",
    "**2. Agora vamos extrair os links dentro do href (coloque isso dentro do método `parse`):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autores_urls = response.css('div.quote > span > a::attr(href)').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Como resposta, recebemos uma List de Strings, essas Strings são sobre todos os autores que estão na página.\n",
    "**3. Agora vamos criar um loop para varrer essa Lista e acessar o site (também dentro do `parse`).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in autores_urls:\n",
    "            url = response.urljoin(url)\n",
    "            yield scrapy.Request(url=url, callback=self.parse_detalhes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código ta bem parecido com o que fizemos no [Notebook anterior](https://github.com/DwarfThief/Raspagem-de-dados-para-iniciantes/blob/master/Navegando%20entre%20paginas.ipynb), mas note que no parâmetro de `callback=` eu não coloquei o `self.parse`, em vez disso eu coloquei `self.parse_detalhes`. Sendo assim ele irá para a função `parse_detalhes`, vai ser nessa função que vamos colher os dados.\n",
    "\n",
    "**4. Hora de criar a função `parse_detalhes`.**\n",
    "\n",
    "```Python\n",
    "def parse_detalhes(self, response):\n",
    "```\n",
    "Beleza, agora nos temos nossa função, mas ela ta vazia. Lembrando que essa função foi feita para colher os dados. **Vamos inspecionar**, mas dessa vez temos que inspecionar a página do autor, abra qualquer uma. Temos três informações ai, o *nome*, a *data de aniversário* e a *biografia*.\n",
    "    \n",
    "    4.1. Inspecionando o nome:\n",
    "    Vamos descobrir esse caminho `response.css('h3.author-title::text')`, lembrando que devemos extrair a String, sendo assim, `response.css('h3.author-title::text').extract_first()`.\n",
    "    \n",
    "    4.2. Inspecionando a data de aniversário:\n",
    "    Vamos descobrir esse caminho `response.css('span.author-born-date::text').extract_first()`.\n",
    "    \n",
    "    4.3. Inspecionando a biografia: \n",
    "    Vamos descobrir esse caminho `response.css('div.author-description::text').extract_first()`.\n",
    "    \n",
    "Perfeito! Agora vamos colocar isso dentro de um `yield`, eu fiz dessa maneira: \n",
    "```Python\n",
    "def parse_detalhes(self, response):\n",
    "        yield {\n",
    "            'nome' : response.css('h3.author-title::text').extract_first(),\n",
    "            'aniversario' : response.css('span.author-born-date::text').extract_first(),\n",
    "            'detalhes' : response.css('div.author-description::text').extract_first(),\n",
    "        }\n",
    "```\n",
    "\n",
    "## Nosso código final vai estar assim: \n",
    "\n",
    "```Python\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class AutoresSpider(scrapy.Spider):\n",
    "    name = 'autores'\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        #Colhendo os detalhes dos autores       \n",
    "        autores_urls = response.css('div.quote > span > a::attr(href)').extract()\n",
    "        for url in autores_urls:\n",
    "            url = response.urljoin(url)\n",
    "            yield scrapy.Request(url=url, callback=self.parse_detalhes)\n",
    "\n",
    "        #Navegação entre páginas\n",
    "        proxima_pag = response.urljoin(response.css('li.next > a::attr(href)').extract_first())\n",
    "        yield scrapy.Request( url = proxima_pag, callback = self.parse)\n",
    "\n",
    "    def parse_detalhes(self, response):\n",
    "        yield {\n",
    "            'nome' : response.css('h3.author-title::text').extract_first(),\n",
    "            'aniversario' : response.css('span.author-born-date::text').extract_first(),\n",
    "            'detalhes' : response.css('div.author-description::text').extract_first(),\n",
    "        }\n",
    "        \n",
    "```\n",
    "\n",
    "* Para executar: `scrapy runspider autores.py`\n",
    "* Caso queira colocar os dados em um arquivo, escreva: `scrapy runspider autores.py -o autores.json`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
